{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef0a1e4",
   "metadata": {
    "id": "cef0a1e4"
   },
   "source": [
    "# Affective Polarisation Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d494",
   "metadata": {
    "id": "5c66d494"
   },
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cbfbb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cbfbb3",
    "outputId": "82f0e9a1-0cbc-4677-d6bb-3b407a7e0c57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import own functions written in moralisation classifier notebook (NB II) saved to .py\n",
    "from finalproject_functions import reddit_preprocessing, remove_bad_rows, MyTokenizer\n",
    "\n",
    "# import other required packages:\n",
    "import pandas as pd\n",
    "import regex\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "import emoji\n",
    "import warnings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import embeddingvectorizer\n",
    "from embeddingvectorizer import EmbeddingCountVectorizer, EmbeddingTfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ac887",
   "metadata": {
    "id": "0e1ac887"
   },
   "source": [
    "## Load Datasets Containing Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ac2bf2",
   "metadata": {
    "id": "e7ac2bf2"
   },
   "outputs": [],
   "source": [
    "# load labelled & unlabelled comments:\n",
    "labelled_comments = pd.read_excel(\"labs_labelled_comments.xlsx\")\n",
    "unlabelled_comments = pd.read_csv(\"unlabelled_comments.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff8fc2",
   "metadata": {
    "id": "40ff8fc2"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4c7f28",
   "metadata": {
    "id": "7b4c7f28"
   },
   "outputs": [],
   "source": [
    "# store regular expressions in list to remove desired characters\n",
    "regex_list = [\n",
    "      r\"&[^;]+;\", #remove html character escapes (&amp etc.)\n",
    "      r\"</?\\w[^>]*>\", #remove html tags \n",
    "      r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", # remove links to websites\n",
    "      r\"\\s(www.\\S+)\" # remove links to websites\n",
    "      r\"\\*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55117263",
   "metadata": {
    "id": "55117263"
   },
   "outputs": [],
   "source": [
    "# store original length in object n\n",
    "n = len(labelled_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2bdbe",
   "metadata": {
    "id": "49c2bdbe"
   },
   "source": [
    "### Apply pre-written preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e15e96",
   "metadata": {
    "id": "f1e15e96"
   },
   "outputs": [],
   "source": [
    "# remove unwanted regular expressions stored in regex_list\n",
    "labelled_comments[\"comment_pr\"] = labelled_comments[\"comment\"].apply(lambda x: reddit_preprocessing(x, regex_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be91fcbe",
   "metadata": {
    "id": "be91fcbe"
   },
   "outputs": [],
   "source": [
    "# remove duplicates and NAs\n",
    "labelled_comments = remove_bad_rows(labelled_comments, \"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8f73ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d8f73ce",
    "outputId": "796ea882-855d-4db5-c33a-624ea42128b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 comments were removed after data cleaning\n"
     ]
    }
   ],
   "source": [
    "print(f\"{n-len(labelled_comments)} comments were removed after data cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12f8c8",
   "metadata": {
    "id": "ed12f8c8"
   },
   "source": [
    "## Remove Social Identity-Related Terms from Stopword List:\n",
    "As social identity mechanisms are thought to underly affective polarisation (e.g., Iyengar et al., 2012), in coding the comments, I paid attention to words indicative of social identity dynamics, namely \"us\", \"we\", \"them\", \"they\". Therefore, these terms are removed from the stopword list (see van Atteveld et al., 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec02d7e",
   "metadata": {
    "id": "2ec02d7e"
   },
   "outputs": [],
   "source": [
    "# store SI-related words in word_list\n",
    "word_list = [\"us\", \"we\", \"they\", \"them\"]\n",
    "\n",
    "# create filtered stopwords list:\n",
    "stopwords_filtered = [word for word in word_list if word not in stopwords.words('english')] \n",
    "\n",
    "\n",
    "#Aproach: see https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fff197",
   "metadata": {
    "id": "51fff197"
   },
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf8a5ad",
   "metadata": {
    "id": "0cf8a5ad"
   },
   "outputs": [],
   "source": [
    "# Train Test Split using the preprocessed comments column and the overall morality label. \n",
    "# X_test_f and y_test_f are set aside to test the final model.\n",
    "X_train, X_test_f, y_train, y_test_f = train_test_split(\n",
    "    labelled_comments[\"comment_pr\"],\n",
    "    labelled_comments[\"AP_label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=99)\n",
    "\n",
    "# Split the training data again, this time with test size = .25 to achieve a final split of \n",
    "# 60 training data; 20 validation data (this is where baseline is tested on); 20 final testing data (best model testing)\n",
    "X_train_sec, X_val, y_train_sec, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4151f",
   "metadata": {
    "id": "44a4151f"
   },
   "source": [
    "## Classifier configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d4e96",
   "metadata": {
    "id": "db7d4e96"
   },
   "source": [
    "#### Inspect balance of labelled data to determine best adjustment of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d0b19a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "f4d0b19a",
    "outputId": "e4c0763c-8454-4d3a-8c55-474c0cb4598c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-492d6028-7fb6-4383-b7a2-11db34cc6f7b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>AP_inciv</th>\n",
       "      <th>AP_group</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_pr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-492d6028-7fb6-4383-b7a2-11db34cc6f7b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-492d6028-7fb6-4383-b7a2-11db34cc6f7b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-492d6028-7fb6-4383-b7a2-11db34cc6f7b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          Unnamed: 0  post_id  comment_id  AP_inciv  AP_group  comment  \\\n",
       "AP_label                                                                 \n",
       "0                784      784         784       784       784      784   \n",
       "1                213      213         213       213       213      213   \n",
       "\n",
       "          comment_pr  \n",
       "AP_label              \n",
       "0                784  \n",
       "1                213  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_comments.groupby([\"AP_label\"]).count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de53cd3",
   "metadata": {
    "id": "9de53cd3"
   },
   "source": [
    "As the classes are unbalanced (under-representations of comments labelled as containing indicators of affective polarisation) the classifiers are adjusted (setting parameter class_weight to \"balanced\" by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6d7d2",
   "metadata": {
    "id": "9ae6d7d2"
   },
   "source": [
    "#### Define Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922f588d",
   "metadata": {
    "id": "922f588d"
   },
   "outputs": [],
   "source": [
    "# assign the previously saved class \"MyTokenizer()\" to the object \"mytokenizer\" to use in the configurations\n",
    "mytokenizer=MyTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214d04d",
   "metadata": {
    "id": "a214d04d"
   },
   "source": [
    "#### Store Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3dff52",
   "metadata": {
    "id": "7c3dff52"
   },
   "outputs": [],
   "source": [
    "# Store configurations in \"configurations\" object\n",
    "configurations_AP=[('NB_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_filtered), ComplementNB()),\n",
    "('NB_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_filtered), ComplementNB()), \n",
    "('LR_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                              stop_words = stopwords_filtered), LogisticRegression(solver='liblinear',\n",
    "                                                                                           class_weight = 'balanced')),\n",
    "('LR_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_filtered), LogisticRegression(solver='liblinear', \n",
    "                                                                                          class_weight = 'balanced')),\n",
    "('SVM_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_filtered), SVC(gamma = 'scale', \n",
    "                                                                                            class_weight = 'balanced')),\n",
    "('SVM_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_filtered),SVC(gamma = 'scale', \n",
    "                                                                          class_weight = 'balanced')), \n",
    "('RF_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_filtered), RandomForestClassifier(class_weight = 'balanced')), \n",
    "('RF_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_filtered) , RandomForestClassifier(class_weight = 'balanced'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943b12a",
   "metadata": {
    "id": "d943b12a"
   },
   "source": [
    "#### Classification& Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc29eed",
   "metadata": {
    "id": "7dc29eed"
   },
   "outputs": [],
   "source": [
    "# Classification function \n",
    "def classification(x):\n",
    "  for name, vectorizer, classifier in x:\n",
    "      trans_X_train_sec = vectorizer.fit_transform(X_train_sec)\n",
    "      trans_X_val = vectorizer.transform(X_val)\n",
    "      classifier.fit(trans_X_train_sec, y_train_sec)\n",
    "      pred_y_sm = classifier.predict(trans_X_val)\n",
    "      print(f\"Classification Report for {name}:\\n\")\n",
    "      print(classification_report(y_val, pred_y_sm))\n",
    "      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca33e0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ca33e0d",
    "outputId": "709918a0-6492-4069-93ef-141e7db9ae3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for NB_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84       152\n",
      "           1       0.38      0.19      0.25        48\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.58      0.54      0.54       200\n",
      "weighted avg       0.68      0.73      0.69       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for NB_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86       152\n",
      "           1       0.50      0.06      0.11        48\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.63      0.52      0.49       200\n",
      "weighted avg       0.70      0.76      0.68       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       152\n",
      "           1       0.48      0.42      0.44        48\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.65      0.64      0.64       200\n",
      "weighted avg       0.74      0.75      0.74       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       152\n",
      "           1       0.47      0.46      0.46        48\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.74      0.74      0.74       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86       152\n",
      "           1       0.56      0.29      0.38        48\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.68      0.61      0.62       200\n",
      "weighted avg       0.75      0.78      0.75       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.87       152\n",
      "           1       0.67      0.08      0.15        48\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.72      0.54      0.51       200\n",
      "weighted avg       0.75      0.77      0.69       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RF_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       152\n",
      "           1       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.38      0.50      0.43       200\n",
      "weighted avg       0.58      0.76      0.66       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RF_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       152\n",
      "           1       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.38      0.50      0.43       200\n",
      "weighted avg       0.58      0.76      0.66       200\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "classification(configurations_AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb99fa",
   "metadata": {
    "id": "a0fb99fa"
   },
   "source": [
    "## Gridsearch on Best Performing Models: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1acc5",
   "metadata": {
    "id": "eeb1acc5"
   },
   "source": [
    "The following models were selected to perform a gridsearch, as their f1-score was the highest (and closest to 0.5). Because both precision and recall are deemed central for the classification of affective polarisation indicators in online language, LR_CountV seems to be the better model overall, however, given different hyperparameters this may change.\n",
    "- LR_Count\n",
    "- LR_Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d493a",
   "metadata": {
    "id": "ba8d493a"
   },
   "source": [
    "#### Gridsearch for LR_Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a150d75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "7a150d75",
    "outputId": "f8449d40-b446-4490-ecc5-a9f96daf14aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        CountVectorizer(stop_words=['us'],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <finalproject_functions.MyTokenizer object at 0x7f881169bd90>>)),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2'],\n",
       "                         'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('LR_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                              stop_words = stopwords_filtered), LogisticRegression(solver='liblinear',\n",
    "                                                                                           class_weight = 'balanced')),\n",
    "\n",
    "\n",
    "# Define pipeline with the model, defining the vectorizer and classifier \n",
    "LR_CountV_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                              stop_words = stopwords_filtered)),\n",
    "        (\"classifier\", LogisticRegression(solver='liblinear', class_weight = 'balanced')),\n",
    "            ]\n",
    "           )\n",
    "# Specify ranges of values for hyperparameters to test for the model supplied to the pipeline\n",
    "LR_CountV_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10],\n",
    "    \"classifier__C\":[0.01, 0.1, 1, 10, 100], \n",
    "    \"classifier__penalty\":[\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "# perform gridsearch\n",
    "search_LR_CountV = GridSearchCV(\n",
    "    estimator = LR_CountV_pipe, n_jobs=-1, param_grid=LR_CountV_grid, scoring=\"f1\", cv=10)\n",
    "search_LR_CountV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48d075f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48d075f0",
    "outputId": "39950c39-e3db-4a52-bb69-67891831d870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.443\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_LR_CountV.best_params_}\")\n",
    "print(f\"Best score: {round(search_LR_CountV.best_score_,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Ax4r18WGHG1K",
   "metadata": {
    "id": "Ax4r18WGHG1K"
   },
   "outputs": [],
   "source": [
    "LR_CountV_model = search_LR_CountV.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b729e",
   "metadata": {
    "id": "247b729e"
   },
   "source": [
    "#### Gridsearch for LR_Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d5af70b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "1d5af70b",
    "outputId": "d4f52e18-965e-4aaa-c68b-d19b2965217c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;us&#x27;],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;finalproject_functions.MyTokenizer object at 0x7f881169bd90&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words=['us'],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <finalproject_functions.MyTokenizer object at 0x7f881169bd90>>)),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2'],\n",
       "                         'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_Tfidf_GS_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                          stop_words = stopwords_filtered)),\n",
    "        (\"classifier\", LogisticRegression(solver='liblinear',\n",
    "                                          class_weight = 'balanced')),\n",
    "            ]\n",
    "           )\n",
    "LR_Tfidf_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10],\n",
    "    \"classifier__C\":[0.01, 0.1, 1, 10, 100], \n",
    "    \"classifier__penalty\":[\"l1\", \"l2\"]\n",
    "}\n",
    "search_LR_Tfidf = GridSearchCV(\n",
    "    estimator = LR_Tfidf_GS_pipe, n_jobs=-1, param_grid=LR_Tfidf_grid, scoring=\"f1\", cv=10)\n",
    "search_LR_Tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c086f27a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c086f27a",
    "outputId": "9d02c9e8-bbfe-48ea-e8ff-e7ed0c5802a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.457\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_LR_Tfidf.best_params_}\")\n",
    "print(f\"Best score: {round(search_LR_Tfidf.best_score_,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "MJyaeWFnHM3h",
   "metadata": {
    "id": "MJyaeWFnHM3h"
   },
   "outputs": [],
   "source": [
    "# Save best model with best hyperparameter settings\n",
    "LR_Tfidf_model = search_LR_Tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b9028",
   "metadata": {
    "id": "634b9028"
   },
   "source": [
    "Based on the rounding, it seems that the LR_TFidf is the better classifier, but both perform approximately the same. Therefore, Word Embeddings will be added to both, to see if this makes a difference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4feee5d",
   "metadata": {
    "id": "e4feee5d"
   },
   "source": [
    "## Adding Word Embeddings to Best Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1cda567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1cda567",
    "outputId": "bc2309d6-a5be-4ee6-bd6a-0ad4f7d984fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained word embeddings on Wikipedia Corpus\n",
    "wv = api.load('glove-wiki-gigaword-300')\n",
    "wv_model = dict(zip(wv.index_to_key, wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9af3f",
   "metadata": {
    "id": "d2c9af3f"
   },
   "source": [
    "Adding best parameters into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aOUxG0PtyH8E",
   "metadata": {
    "id": "aOUxG0PtyH8E"
   },
   "outputs": [],
   "source": [
    "# Store the pre-trained word embeddings and Count or Tfidf vectorizers in the configurations list to get the \n",
    "# classification report in the next step\n",
    "configurations_AP_emb=[ \n",
    "('LR_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'),\n",
    " LogisticRegression(solver='liblinear',\n",
    "                    class_weight = 'balanced')),\n",
    "('LR_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'),\n",
    " LogisticRegression(solver='liblinear', class_weight = 'balanced')),\n",
    "('SVM_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'),\n",
    " SVC(gamma = 'scale', class_weight = 'balanced')),\n",
    "('SVM_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'),\n",
    " SVC(gamma = 'scale', class_weight = 'balanced')), \n",
    "('RF_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'),\n",
    " RandomForestClassifier(class_weight = 'balanced')), \n",
    "('RF_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'),\n",
    " RandomForestClassifier(class_weight = 'balanced'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "AR6HuGs5yivj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AR6HuGs5yivj",
    "outputId": "06781ed4-db92-48fe-d404-0f0dc771635d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78       152\n",
      "           1       0.39      0.58      0.47        48\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.62      0.65      0.62       200\n",
      "weighted avg       0.74      0.69      0.70       200\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for LR_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       152\n",
      "           1       0.40      0.52      0.45        48\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.61      0.64      0.62       200\n",
      "weighted avg       0.73      0.69      0.71       200\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for SVM_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.72      0.78       152\n",
      "           1       0.41      0.62      0.50        48\n",
      "\n",
      "    accuracy                           0.69       200\n",
      "   macro avg       0.63      0.67      0.64       200\n",
      "weighted avg       0.75      0.69      0.71       200\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for SVM_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       152\n",
      "           1       0.47      0.58      0.52        48\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.67      0.69      0.67       200\n",
      "weighted avg       0.77      0.74      0.75       200\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for RF_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       152\n",
      "           1       1.00      0.02      0.04        48\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.88      0.51      0.45       200\n",
      "weighted avg       0.82      0.77      0.67       200\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for RF_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87       152\n",
      "           1       1.00      0.02      0.04        48\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.88      0.51      0.45       200\n",
      "weighted avg       0.82      0.77      0.67       200\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification (configurations_AP_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O1e0JFMozJq-",
   "metadata": {
    "id": "O1e0JFMozJq-"
   },
   "source": [
    "Using the word embeddings, the best models identified are SVM_Tfidf, achieving f1=0.52. With BOW and gridsearch, the best model is LR_Tfidf with f1=0.46. Therefore, the final baseline model selected is SVM using word embeddings and a count vectorizer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BlmORVP4xI6q",
   "metadata": {
    "id": "BlmORVP4xI6q"
   },
   "source": [
    "## Fitting the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "LqpowhaKxCgv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqpowhaKxCgv",
    "outputId": "7eb9f67f-d90d-4473-fd00-619ec66d4142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82       164\n",
      "           1       0.35      0.58      0.44        36\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.62      0.67      0.63       200\n",
      "weighted avg       0.80      0.73      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the best model as identified in the previous step in the pipe\n",
    "em_pipe = Pipeline([('SVM_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean')),\n",
    "                    ('svm_svc', \n",
    "                     SVC(gamma = 'scale', class_weight = 'balanced'))\n",
    "])\n",
    "\n",
    "# fit the model to the training data\n",
    "em_pipe.fit(X_train_sec, y_train_sec)\n",
    "\n",
    "# predict the labels of the final set aside testing data\n",
    "y_pred = em_pipe.predict(X_test_f)\n",
    "\n",
    "# get model performance metrics\n",
    "print(metrics.classification_report(y_test_f, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uGXRWtq8yFh5",
   "metadata": {
    "id": "uGXRWtq8yFh5"
   },
   "source": [
    "## Clean Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97473669",
   "metadata": {},
   "source": [
    "This step was added after comparing all models (including BERT). Before applying the best model to label the comments, this data needs to be preprocessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04oZ224hyKD-",
   "metadata": {
    "id": "04oZ224hyKD-"
   },
   "outputs": [],
   "source": [
    "# apply preprocessing functions removing regular expressions stored in the regex list\n",
    "unlabelled_comments[\"comment\"] = unlabelled_comments[\"comment\"].apply(lambda x: reddit_preprocessing(x, regex_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "SdUM33skyNjd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdUM33skyNjd",
    "outputId": "62900674-870d-4af4-f3b1-c6a557f7f77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 titles were removed\n"
     ]
    }
   ],
   "source": [
    "# apply function to remove duplicate comments and missing values.\n",
    "unlabelled_comments = remove_bad_rows(unlabelled_comments, \"comment\")\n",
    "print(f\"{n_u-len(unlabelled_comments)} titles were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3IYZdRu5yd4z",
   "metadata": {
    "id": "3IYZdRu5yd4z"
   },
   "source": [
    "## Final Model Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aKyjmX0-yZxy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aKyjmX0-yZxy",
    "outputId": "f510acd2-691c-4d82-9e3c-21acd6f46580"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9180e5d4-caaf-4365-b216-4597b9f44d09\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>AP_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>flgxp</td>\n",
       "      <td>c1gtf8n</td>\n",
       "      <td>I don't advocate the death penalty.  But this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flgxp</td>\n",
       "      <td>c1gtjcm</td>\n",
       "      <td>Did anybody else see [this video](/david-neiwe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>flgxp</td>\n",
       "      <td>c1gtlbj</td>\n",
       "      <td>I've been following this case for a while. I'm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>flgxp</td>\n",
       "      <td>c1gtnqs</td>\n",
       "      <td>Good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>flgxp</td>\n",
       "      <td>c1gtnuc</td>\n",
       "      <td>Fry that bitch!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9180e5d4-caaf-4365-b216-4597b9f44d09')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9180e5d4-caaf-4365-b216-4597b9f44d09 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9180e5d4-caaf-4365-b216-4597b9f44d09');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0 post_id comment_id  \\\n",
       "0           0   flgxp    c1gtf8n   \n",
       "1           1   flgxp    c1gtjcm   \n",
       "2           2   flgxp    c1gtlbj   \n",
       "3           4   flgxp    c1gtnqs   \n",
       "4           5   flgxp    c1gtnuc   \n",
       "\n",
       "                                             comment  AP_labels  \n",
       "0  I don't advocate the death penalty.  But this ...          0  \n",
       "1  Did anybody else see [this video](/david-neiwe...          0  \n",
       "2  I've been following this case for a while. I'm...          0  \n",
       "3                                              Good.          0  \n",
       "4                                    Fry that bitch!          0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use model stored as \"em_pipe\" to predict the labels in the unlabelled comments dataset\n",
    "unlabelled_comments[\"AP_labels\"] = em_pipe.predict(unlabelled_comments[\"comment\"])\n",
    "unlabelled_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sn9VePWPymMh",
   "metadata": {
    "id": "sn9VePWPymMh"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "with open(\"Polarisation_Model.pkl\", mode=\"wb\") as f:\n",
    "    pickle.dump(em_pipe, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "AiaS_peMyugP",
   "metadata": {
    "id": "AiaS_peMyugP"
   },
   "outputs": [],
   "source": [
    "# Save unlabelled posts to .csv\n",
    "unlabelled_comments.to_csv(\"model_labelled_comments.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
