{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Fa1n7HNtEc"
   },
   "source": [
    "# Morality Baseline Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRATcoivYIlN"
   },
   "source": [
    "This script proceeds in the following steps: \n",
    "- Data preprocessing and cleaning \n",
    "- Fitting baseline classifiers \n",
    "- Gridsearch on best baseline classifiers \n",
    "- Train all baseline classifiers with word embeddings  \n",
    "- Get best performing candidate model and apply to final testing dataset set aside in the train-test-validation split\n",
    "- This step was later added after all models were run, including BERT, and the best model was determined to be a word embedding model:Use the best performing classifier to label the unlabelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5RkgjR-cLtV"
   },
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-pvUwXNcPcD",
    "outputId": "3f814cab-0f41-495a-e90d-1583f4f74e98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "import emoji\n",
    "import warnings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import embeddingvectorizer\n",
    "from embeddingvectorizer import EmbeddingCountVectorizer, EmbeddingTfidfVectorizer\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3iCXU5eegXd"
   },
   "source": [
    "## Load Datasets of Reddit Posts to r/news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IZ5y0QdgXQL"
   },
   "source": [
    "#### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XKjRs1IvejWM"
   },
   "outputs": [],
   "source": [
    "# load labelled & unlabelled posts:\n",
    "labelled_posts = pd.read_excel(\"labs_labelled_posts_new.xlsx\")\n",
    "unlabelled_posts = pd.read_csv(\"unlabelled_posts_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EATf5DAXiExW"
   },
   "source": [
    "## Data Preprocessing Functions to Compute Baseline Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R4XCMd--VFvH"
   },
   "outputs": [],
   "source": [
    "# this function takes the text and an object with regex stored in it as input\n",
    "def reddit_preprocessing(x, listofregex):\n",
    "    # loops over regular expressions stored in the list \n",
    "  for expression in listofregex:\n",
    "    # replaces the respective regex with \"\" if it is identified\n",
    "    x = re.sub(expression, \"\", x)\n",
    "    # replaces any emojis with \"\"\n",
    "  x = emoji.replace_emoji(x, replace=\"\") # remove emojis from the text\n",
    "    # returns text without the patterns identified in the regular expressions\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D_fZZBRIiL_D"
   },
   "outputs": [],
   "source": [
    "# Function to remove duplicate text and missing data (e.g., posts or comments with differen IDs but with the same content)\n",
    "def remove_bad_rows(df, textcolumn): \n",
    "  df = df.drop_duplicates(subset = textcolumn).dropna()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjRhkN77VFvI"
   },
   "source": [
    "## Custom Tokenizer for Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rINuVpRYVFvI"
   },
   "outputs": [],
   "source": [
    "# Self-made tokenizer: \n",
    "class MyTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        result = [] \n",
    "        word_pattern = r\"\\p{letter}\" # match any unicode character that is a letter, therefore strips special chatacters\n",
    "        tokens = nltk.word_tokenize(text, language = 'english') # use word_tokenize (uses improved TreebankWordTokenizer & PunktSentenceTokenizer()\n",
    "        tokens = [e for e in tokens if regex.search(word_pattern, e)]\n",
    "        result += tokens    \n",
    "        return result\n",
    "\n",
    "mytokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gg0JLmW5wtv"
   },
   "source": [
    "# Moralised Frames in Posts â€“ Classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ut1DMGI0VFvJ"
   },
   "outputs": [],
   "source": [
    "n = len(labelled_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE8maUB7VFvJ"
   },
   "source": [
    "## Apply Preprocessing Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v4Gxb7zjVFvJ"
   },
   "outputs": [],
   "source": [
    "# store regular expressions in list to remove desired characters\n",
    "regex_list = [\n",
    "      r\"&[^;]+;\", #remove html character escapes (&amp etc.)\n",
    "      r\"</?\\w[^>]*>\", #remove html tags \n",
    "      r\"https?://[\\w\\.]+\\b|www\\.[\\w\\.]+\\b\", # remove links to websites\n",
    "      r\"\\s(www.\\S+)\" # remove links to websites\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "trcgOA--VFvK"
   },
   "outputs": [],
   "source": [
    "# apply preprocessing to title column\n",
    "labelled_posts[\"title_pr_c\"] = labelled_posts[\"title\"].apply(lambda x: reddit_preprocessing(x, regex_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bi8eIAzzVFvK",
    "outputId": "dfa27308-7f41-4b26-8d50-b05159a96475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 titles were removed\n"
     ]
    }
   ],
   "source": [
    "labelled_posts = remove_bad_rows(labelled_posts, \"title\")\n",
    "print(f\"{n-len(labelled_posts)} titles were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChrVVhtjFgm"
   },
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vqSbgprHjIyw"
   },
   "outputs": [],
   "source": [
    "# Train Test Split using the preprocessed title column and the overall morality label. \n",
    "# X_test_f and y_test_f are set aside to test the final model.\n",
    "X_train, X_test_f, y_train, y_test_f = train_test_split(\n",
    "    labelled_posts[\"title_pr_c\"],\n",
    "    labelled_posts[\"moral_label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=99)\n",
    "\n",
    "# Split the training data again, this time with test size = .25 to achieve a final split of \n",
    "# 60 training data; 20 validation data (this is where baseline is tested on); 20 final testing data (best model testing)\n",
    "X_train_sec, X_val, y_train_sec, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OakvKX68iVd"
   },
   "source": [
    "## Extending Stopword List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jxPPvnX3B0b4"
   },
   "outputs": [],
   "source": [
    "# Extend stopword list based on a warning that was raised when running the models\n",
    "# Some of the stopwords contained in the warning (e.g., \"must\") \n",
    "# were deemed important for moralisation and were not to included in the stopword lists\n",
    "stopwords_ext = [\"'re\", \"'s\", 'sha', 'wo'] + stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcydfJVojsAX"
   },
   "source": [
    "## Baseline SVM Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IeP_FPw5sh2"
   },
   "source": [
    "#### Inspect balance of labelled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "MobTtIrO1B_T",
    "outputId": "5aabee57-eb98-4d74-e962-2005f33951f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cdf10cee-6590-41bd-ab3e-9ca0a255d439\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label_threat</th>\n",
       "      <th>label_vict</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>title_pr_c</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moral_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdf10cee-6590-41bd-ab3e-9ca0a255d439')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cdf10cee-6590-41bd-ab3e-9ca0a255d439 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cdf10cee-6590-41bd-ab3e-9ca0a255d439');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "             Unnamed: 0  label_threat  label_vict  post_id  title  date  \\\n",
       "moral_label                                                               \n",
       "0                   715           715         715      715    715   715   \n",
       "1                   266           266         266      266    266   266   \n",
       "\n",
       "             title_pr_c  \n",
       "moral_label              \n",
       "0                   715  \n",
       "1                   266  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect balance of labelled data: \n",
    "labelled_posts.groupby([\"moral_label\"]).count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aBKQ_BM4xpP"
   },
   "source": [
    "As the classes are unbalanced (under-representations of moral frames in titles) the classifiers are adjusted (using ComplementNB() or adjusting the parameter class_weight to \"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTeuojaa5yWn"
   },
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UVYcjGIwjnF4"
   },
   "outputs": [],
   "source": [
    "# Store configurations in \"configurations\" object\n",
    "\n",
    "configurations=[('NB_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_ext), ComplementNB()),\n",
    "('NB_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_ext), ComplementNB()), \n",
    "('LR_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                              stop_words = stopwords_ext), LogisticRegression(solver='liblinear',\n",
    "                                                                                           class_weight = 'balanced')),\n",
    "('LR_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_ext), LogisticRegression(solver='liblinear', \n",
    "                                                                                          class_weight = 'balanced')),\n",
    "('SVM_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_ext), SVC(gamma = 'scale', \n",
    "                                                                                            class_weight = 'balanced')),\n",
    "('SVM_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_ext),SVC(gamma = 'scale', \n",
    "                                                                          class_weight = 'balanced')), \n",
    "('RF_CountV', CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_ext), RandomForestClassifier(class_weight = 'balanced')), \n",
    "('RF_Tfidf', TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                             stop_words = stopwords_ext) , RandomForestClassifier(class_weight = 'balanced'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jzhaX1awMPPU"
   },
   "outputs": [],
   "source": [
    "# Classification function\n",
    "def classification(x):\n",
    "  for name, vectorizer, classifier in x:\n",
    "      trans_X_train_sec = vectorizer.fit_transform(X_train_sec)\n",
    "      trans_X_val = vectorizer.transform(X_val)\n",
    "      classifier.fit(trans_X_train_sec, y_train_sec)\n",
    "      pred_y_sm = classifier.predict(trans_X_val)\n",
    "      print(f\"Classification Report for {name}:\\n\")\n",
    "      print(classification_report(y_val, pred_y_sm))\n",
    "      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HCTF3OUOB9V",
    "outputId": "80c1b8ab-ba17-46ae-b213-9ad4ff4b9a4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for NB_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       143\n",
      "           1       0.53      0.47      0.50        53\n",
      "\n",
      "    accuracy                           0.74       196\n",
      "   macro avg       0.67      0.66      0.66       196\n",
      "weighted avg       0.74      0.74      0.74       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for NB_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       143\n",
      "           1       0.50      0.43      0.46        53\n",
      "\n",
      "    accuracy                           0.73       196\n",
      "   macro avg       0.65      0.64      0.64       196\n",
      "weighted avg       0.72      0.73      0.72       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       143\n",
      "           1       0.59      0.43      0.50        53\n",
      "\n",
      "    accuracy                           0.77       196\n",
      "   macro avg       0.70      0.66      0.67       196\n",
      "weighted avg       0.75      0.77      0.75       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       143\n",
      "           1       0.52      0.49      0.50        53\n",
      "\n",
      "    accuracy                           0.74       196\n",
      "   macro avg       0.67      0.66      0.66       196\n",
      "weighted avg       0.74      0.74      0.74       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       143\n",
      "           1       0.59      0.36      0.45        53\n",
      "\n",
      "    accuracy                           0.76       196\n",
      "   macro avg       0.69      0.63      0.65       196\n",
      "weighted avg       0.74      0.76      0.74       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85       143\n",
      "           1       0.65      0.25      0.36        53\n",
      "\n",
      "    accuracy                           0.76       196\n",
      "   macro avg       0.71      0.60      0.60       196\n",
      "weighted avg       0.74      0.76      0.72       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RF_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       143\n",
      "           1       1.00      0.13      0.23        53\n",
      "\n",
      "    accuracy                           0.77       196\n",
      "   macro avg       0.88      0.57      0.55       196\n",
      "weighted avg       0.82      0.77      0.69       196\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RF_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86       143\n",
      "           1       0.89      0.15      0.26        53\n",
      "\n",
      "    accuracy                           0.77       196\n",
      "   macro avg       0.82      0.57      0.56       196\n",
      "weighted avg       0.79      0.77      0.70       196\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline classification task & return report:\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "classification(configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaRibkTWVFvN"
   },
   "source": [
    "Both precision and recall were deemed important, as the classifier should both \n",
    "The best performing models were: \n",
    "- NB_CountV\n",
    "- NB_Tfidf\n",
    "- LR_CountV\n",
    "- LR_Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_336Wg0VFvN"
   },
   "source": [
    "## Grid search for the best models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "tpeyAjnBVFvN",
    "outputId": "5f2e5d5a-bcf6-465e-e847-189f8e554be4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;,\n",
       "                                             &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                             &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                             &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                             &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                             &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                             &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;, ComplementNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;,\n",
       "                            &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;,\n",
       "                            &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                            &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                            &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ComplementNB</label><div class=\"sk-toggleable__content\"><pre>ComplementNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        CountVectorizer(stop_words=[\"'re\", \"'s\",\n",
       "                                                                    'sha', 'wo',\n",
       "                                                                    'i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself', ...],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <__main__.MyTokenizer object at 0x7fc5c74d6e00>>)),\n",
       "                                       ('classifier', ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore warnings for better readability\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "NB_CountV_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                              stop_words = stopwords_ext)),\n",
    "        (\"classifier\", ComplementNB()),\n",
    "            ]\n",
    "           )\n",
    "NB_CountV_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10]\n",
    "}\n",
    "search_NB_CountV = GridSearchCV(\n",
    "    estimator = NB_CountV_pipe, n_jobs=-1, param_grid=NB_CountV_grid, scoring=\"f1\", cv=10)\n",
    "search_NB_CountV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO6k-IGeVFvN",
    "outputId": "e16f4ed6-da1c-4d3b-e902-f0463781cb0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 2)}\n",
      "Best score: 0.4841\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_NB_CountV.best_params_}\")\n",
    "print(f\"Best score: {round(search_NB_CountV.best_score_,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "PihD2C27VFvN",
    "outputId": "41fb136d-e46b-4f93-cb37-47efff736c74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;,\n",
       "                                             &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                             &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                             &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                             &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                             &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                             &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;, ComplementNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;,\n",
       "                            &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;,\n",
       "                            &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                            &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                            &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ComplementNB</label><div class=\"sk-toggleable__content\"><pre>ComplementNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words=[\"'re\", \"'s\",\n",
       "                                                                    'sha', 'wo',\n",
       "                                                                    'i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself', ...],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <__main__.MyTokenizer object at 0x7fc5c74d6e00>>)),\n",
       "                                       ('classifier', ComplementNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "NB_Tfidf_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                          stop_words = stopwords_ext)),\n",
    "        (\"classifier\", ComplementNB()),\n",
    "            ]\n",
    "           )\n",
    "NB_Tfidf_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10]\n",
    "}\n",
    "search_NB_Tfidf = GridSearchCV(\n",
    "    estimator = NB_Tfidf_pipe, n_jobs=-1, param_grid=NB_Tfidf_grid, scoring=\"f1\", cv=10)\n",
    "search_NB_Tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bVJK2P8VFvO",
    "outputId": "b901d456-9550-4cd1-d333-3c4c0c4ff750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vectorizer__max_df': 0.75, 'vectorizer__min_df': 10, 'vectorizer__ngram_range': (1, 2)}\n",
      "Best score: 0.4707\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_NB_Tfidf.best_params_}\")\n",
    "print(f\"Best score: {round(search_NB_Tfidf.best_score_,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "5ZnRZ3YIVFvO",
    "outputId": "0c489e49-2766-46dc-dd39-b8b06299bed7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;,\n",
       "                                             &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                             &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                             &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                             &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                             &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                             &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;,\n",
       "                            &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;,\n",
       "                            &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                            &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                            &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        CountVectorizer(stop_words=[\"'re\", \"'s\",\n",
       "                                                                    'sha', 'wo',\n",
       "                                                                    'i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself', ...],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <__main__.MyTokenizer object at 0x7fc5c74d6e00>>)),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2'],\n",
       "                         'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "LR_Count_GS_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", CountVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                              stop_words = stopwords_ext)),\n",
    "        (\"classifier\", LogisticRegression(solver='liblinear',\n",
    "                                          class_weight = 'balanced')),\n",
    "            ]\n",
    "           )\n",
    "LR_Count_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10],\n",
    "    \"classifier__C\":[0.01, 0.1, 1, 10, 100], \n",
    "    \"classifier__penalty\":[\"l1\", \"l2\"]\n",
    "}\n",
    "search_LR_Count = GridSearchCV(\n",
    "    estimator = LR_Count_GS_pipe, n_jobs=-1, param_grid=LR_Count_grid, scoring=\"f1\", cv=10)\n",
    "search_LR_Count.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_i2sBuHVFvO",
    "outputId": "5b72db6b-96d0-47e8-de71-10cd999ffa0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.4967\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_LR_Count.best_params_}\")\n",
    "print(f\"Best score: {round(search_LR_Count.best_score_,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "__w3sFfWZYch"
   },
   "outputs": [],
   "source": [
    "# Store best model with best hyperparameters\n",
    "LR_Count_model = search_LR_Count.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "rAINOj_6VFvO",
    "outputId": "21485a31-80a4-4e44-ea05-f48a58eae010"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;,\n",
       "                                                                    &#x27;sha&#x27;, &#x27;wo&#x27;,\n",
       "                                                                    &#x27;i&#x27;, &#x27;me&#x27;,\n",
       "                                                                    &#x27;my&#x27;,\n",
       "                                                                    &#x27;myself&#x27;,\n",
       "                                                                    &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                                                    &#x27;ours&#x27;,\n",
       "                                                                    &#x27;ourselves&#x27;,\n",
       "                                                                    &#x27;you&#x27;,\n",
       "                                                                    &quot;you&#x27;re&quot;,\n",
       "                                                                    &quot;you&#x27;ve&quot;,\n",
       "                                                                    &quot;you&#x27;ll&quot;,\n",
       "                                                                    &quot;you&#x27;d&quot;,\n",
       "                                                                    &#x27;your&#x27;,\n",
       "                                                                    &#x27;yours&#x27;,\n",
       "                                                                    &#x27;yourself&#x27;,\n",
       "                                                                    &#x27;yourselves&#x27;,\n",
       "                                                                    &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                                    &#x27;his&#x27;,\n",
       "                                                                    &#x27;himself&#x27;,\n",
       "                                                                    &#x27;she&#x27;,\n",
       "                                                                    &quot;she&#x27;s&quot;,\n",
       "                                                                    &#x27;her&#x27;,\n",
       "                                                                    &#x27;hers&#x27;,\n",
       "                                                                    &#x27;herself&#x27;, ...],\n",
       "                                                        tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;classifier__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.5, 0.75, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [0, 5, 10],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;,\n",
       "                                             &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;,\n",
       "                                             &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;,\n",
       "                                             &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                                             &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                                             &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;,\n",
       "                                             &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                             &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                                 tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&quot;&#x27;re&quot;, &quot;&#x27;s&quot;, &#x27;sha&#x27;, &#x27;wo&#x27;, &#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;,\n",
       "                            &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;,\n",
       "                            &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;,\n",
       "                            &#x27;yourself&#x27;, &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                            &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, ...],\n",
       "                tokenizer=&lt;bound method MyTokenizer.tokenize of &lt;__main__.MyTokenizer object at 0x7fc5c74d6e00&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words=[\"'re\", \"'s\",\n",
       "                                                                    'sha', 'wo',\n",
       "                                                                    'i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself', ...],\n",
       "                                                        tokenizer=<bound method MyTokenizer.tokenize of <__main__.MyTokenizer object at 0x7fc5c74d6e00>>)),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2'],\n",
       "                         'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
       "                         'vectorizer__min_df': [0, 5, 10],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "LR_Tfidf_GS_pipe = Pipeline(\n",
    "    steps=[(\"vectorizer\", TfidfVectorizer(tokenizer = mytokenizer.tokenize,\n",
    "                                          stop_words = stopwords_ext)),\n",
    "        (\"classifier\", LogisticRegression(solver='liblinear',\n",
    "                                          class_weight = 'balanced')),\n",
    "            ]\n",
    "           )\n",
    "LR_Tfidf_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vectorizer__max_df\": [0.5, 0.75, 1.0],\n",
    "    \"vectorizer__min_df\": [0, 5, 10],\n",
    "    \"classifier__C\":[0.01, 0.1, 1, 10, 100], \n",
    "    \"classifier__penalty\":[\"l1\", \"l2\"]\n",
    "}\n",
    "search_LR_Tfidf = GridSearchCV(\n",
    "    estimator = LR_Tfidf_GS_pipe, n_jobs=-1, param_grid=LR_Tfidf_grid, scoring=\"f1\", cv=10)\n",
    "search_LR_Tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NW4T-5nbVFvg",
    "outputId": "98dad8ec-6fb2-45cd-f0a2-914b45ad1ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__C': 1, 'classifier__penalty': 'l2', 'vectorizer__max_df': 0.75, 'vectorizer__min_df': 0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.5028\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {search_LR_Tfidf.best_params_}\")\n",
    "print(f\"Best score: {round(search_LR_Tfidf.best_score_,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krTyPOPIkqua"
   },
   "source": [
    "## Add word embeddings and examine improvements in baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7oMsPkKkvFR",
    "outputId": "4d32a01d-d0ef-48cf-cf22-e60a1d654d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.9% 375.9/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained word embeddings on google news.\n",
    "wv = api.load('glove-wiki-gigaword-300')\n",
    "wv_model = dict(zip(wv.index_to_key, wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vUhTFqQcVFvg"
   },
   "outputs": [],
   "source": [
    "# Store configurations in \"configurations\" object, including word embeddings and the count/tfidf vectorizers\n",
    "\n",
    "configurations_emb=[('LR_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'), LogisticRegression(solver='liblinear',\n",
    "                                                                                           class_weight = 'balanced')),\n",
    "('LR_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'), LogisticRegression(solver='liblinear', \n",
    "                                                                                          class_weight = 'balanced')),\n",
    "('SVM_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'), SVC(gamma = 'scale', \n",
    "                                                                                            class_weight = 'balanced')),\n",
    "('SVM_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'),SVC(gamma = 'scale', \n",
    "                                                                          class_weight = 'balanced')), \n",
    "('RF_CountV', embeddingvectorizer.EmbeddingCountVectorizer(wv_model, operator='mean'), RandomForestClassifier(class_weight = 'balanced')), \n",
    "('RF_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean'), RandomForestClassifier(class_weight = 'balanced'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfSrlxH7oMPT",
    "outputId": "6a19bd3f-784d-4d03-f1c9-c830877e761b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for LR_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       143\n",
      "           1       0.58      0.58      0.58        53\n",
      "\n",
      "    accuracy                           0.78       196\n",
      "   macro avg       0.72      0.72      0.72       196\n",
      "weighted avg       0.78      0.78      0.78       196\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for LR_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82       143\n",
      "           1       0.53      0.55      0.54        53\n",
      "\n",
      "    accuracy                           0.74       196\n",
      "   macro avg       0.68      0.68      0.68       196\n",
      "weighted avg       0.75      0.74      0.75       196\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for SVM_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       143\n",
      "           1       0.64      0.57      0.60        53\n",
      "\n",
      "    accuracy                           0.80       196\n",
      "   macro avg       0.74      0.72      0.73       196\n",
      "weighted avg       0.79      0.80      0.79       196\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for SVM_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.87       143\n",
      "           1       0.65      0.58      0.61        53\n",
      "\n",
      "    accuracy                           0.80       196\n",
      "   macro avg       0.75      0.73      0.74       196\n",
      "weighted avg       0.80      0.80      0.80       196\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for RF_CountV:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87       143\n",
      "           1       1.00      0.23      0.37        53\n",
      "\n",
      "    accuracy                           0.79       196\n",
      "   macro avg       0.89      0.61      0.62       196\n",
      "weighted avg       0.84      0.79      0.74       196\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for RF_Tfidf:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       143\n",
      "           1       1.00      0.21      0.34        53\n",
      "\n",
      "    accuracy                           0.79       196\n",
      "   macro avg       0.89      0.60      0.61       196\n",
      "weighted avg       0.83      0.79      0.73       196\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification & Report:\n",
    "classification(configurations_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngDkkm-LqUMY"
   },
   "source": [
    "On this basis, the best performing model is SVM_Tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O8HPQ_UUA68"
   },
   "source": [
    "# Final Testing of the Best Morality Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPB_du3RNpY5",
    "outputId": "1b0048e0-2454-48a2-b70c-e8d92ea7d555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82       128\n",
      "           1       0.67      0.57      0.61        69\n",
      "\n",
      "    accuracy                           0.75       197\n",
      "   macro avg       0.73      0.71      0.72       197\n",
      "weighted avg       0.75      0.75      0.75       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#specify pipeline with best model identified in the previous step\n",
    "mypipe = Pipeline([('SVM_Tfidf', embeddingvectorizer.EmbeddingTfidfVectorizer(wv_model, operator='mean')),\n",
    "                    ('svm_svc', \n",
    "                     SVC(gamma = 'scale', class_weight = 'balanced'))\n",
    "])\n",
    "\n",
    "# fit the model on the training data\n",
    "mypipe.fit(X_train_sec, y_train_sec)\n",
    "\n",
    "# predict the labels in the final testing data set aside at the start\n",
    "y_pred = mypipe.predict(X_test_f)\n",
    "\n",
    "# get classification report\n",
    "print(metrics.classification_report(y_test_f, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHqjEJfaYIlk"
   },
   "source": [
    "# Clean unlabelled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ViJYSXxzYIlk"
   },
   "outputs": [],
   "source": [
    "n_u = len(unlabelled_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "y1CgNnU3YIll"
   },
   "outputs": [],
   "source": [
    "# apply preprocessing function to title column in unlabelled data\n",
    "unlabelled_posts[\"title\"] = unlabelled_posts[\"title\"].apply(lambda x: reddit_preprocessing(x, regex_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLJ4Tuu2YIll",
    "outputId": "463de279-78ce-45a8-ae9f-f83796a82498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 titles were removed\n"
     ]
    }
   ],
   "source": [
    "# apply removal of NAs and duplicates function to unlabelled posts title column\n",
    "unlabelled_posts = remove_bad_rows(unlabelled_posts, \"title\")\n",
    "print(f\"{n_u-len(unlabelled_posts)} titles were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCLWuMHAYIll"
   },
   "source": [
    "After applying these preprocessing steps, the word embedding vectorizer will apply the rest of the required preprocessing steps for word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwDavMOrYIll"
   },
   "source": [
    "# Label the unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ulX-w3S1YIll"
   },
   "outputs": [],
   "source": [
    "# Use the stored model (\"mypipe\") to predict the moral vs. neutral label in the unlabelled title column\n",
    "unlabelled_posts[\"moral_labels\"] = mypipe.predict(unlabelled_posts[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "0MyP2QTvoJJG",
    "outputId": "d1cebcfc-f13a-4746-d2dd-904248d10cea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d37c56e3-fd88-4923-b3fc-0f7e491a388e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>moral_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6807</td>\n",
       "      <td>8vjh0c</td>\n",
       "      <td>Alabama man arrested after shouting 'womp womp...</td>\n",
       "      <td>2018-07-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10965</td>\n",
       "      <td>nznzfc</td>\n",
       "      <td>U.S. to expand work permits for immigrants who...</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2832</td>\n",
       "      <td>5tavj7</td>\n",
       "      <td>Undocumented Immigrants Arrested Across U.S., ...</td>\n",
       "      <td>2017-02-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5587</td>\n",
       "      <td>7vlzmh</td>\n",
       "      <td>Best Immigration Consultant in Melbourne</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5413</td>\n",
       "      <td>7qyig9</td>\n",
       "      <td>Don Lemon Dumbfounded By Jeff Session's Commen...</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d37c56e3-fd88-4923-b3fc-0f7e491a388e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d37c56e3-fd88-4923-b3fc-0f7e491a388e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d37c56e3-fd88-4923-b3fc-0f7e491a388e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Unnamed: 0 post_id                                              title  \\\n",
       "0        6807  8vjh0c  Alabama man arrested after shouting 'womp womp...   \n",
       "1       10965  nznzfc  U.S. to expand work permits for immigrants who...   \n",
       "2        2832  5tavj7  Undocumented Immigrants Arrested Across U.S., ...   \n",
       "3        5587  7vlzmh           Best Immigration Consultant in Melbourne   \n",
       "4        5413  7qyig9  Don Lemon Dumbfounded By Jeff Session's Commen...   \n",
       "\n",
       "         date  moral_labels  \n",
       "0  2018-07-02             1  \n",
       "1  2021-06-14             1  \n",
       "2  2017-02-11             0  \n",
       "3  2018-02-06             0  \n",
       "4  2018-01-17             0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Gthj4h0UoPKk"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(\"Moralisation_Model.pkl\", mode=\"wb\") as f:\n",
    "    pickle.dump(mypipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "bQrbVFRHpjjy"
   },
   "outputs": [],
   "source": [
    "# Save unlabelled posts to .csv\n",
    "unlabelled_posts.to_csv(\"model_labelled_posts.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
